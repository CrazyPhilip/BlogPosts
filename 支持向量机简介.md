# 支持向量机

支持向量机是一种有监督的二分类模型。

根据样本的分布，可以将支持向量机学习方法分为3种由简至繁的模型：

- 线性可分支持向量机（硬间隔最大化）
- 近似线性可分支持向量机（软间隔最大化）
- 非线性支持向量机（核技巧+软间隔最大化）

## 二分类问题

常常我们会遇到很多二分类问题，比如将筛子中的黄豆和绿豆分开、判断图片中的人物是不是女性等等。

人类在解决这些分类问题时，知道针对不同情况采取不同的方法。比如，如果筛子中的黄豆和绿豆实际上没有参杂在一起，分开就很容易；如果筛子中的黄豆和绿豆参杂在一起，该如何分类呢？如果让计算机来完成分类，它又会如何解决呢？

## 豆子

还是以豆子为例。

如果筛子中的黄豆和绿豆左右分开，区分明显，那么我们可以放一个纸板在中间，两类豆子就分开了。这就是线性可分的例子。

如果筛子中的黄豆和绿豆中间有一点点参杂，那么我们怎么办呢？假如我们可以控制两类豆子，把黄豆左移一点点，绿豆右移一点点，中间放上纸板不就分开了吗。

如果筛子中的黄豆和绿豆完全参杂在一起，那么我们可以把黄豆全部上移，绿豆下移，中间放上纸板不就分开了吗。

实际上，上述三种情况就对应了支持向量机由简至繁的三种模型。

## 最优超平面

支持向量机的运作原理是在样本中寻找一个最优超平面来将样本分开。

如果可以用一个线性函数将样本分开，那么这些样本是线性可分的；如果无法用一个线性函数有效地将样本分开，但可以用一个非线性函数分开样本，那么这些样本是线性不可分的。

这个线性函数在不同维数的空间中表现不同，在一维空间是一个点，二维空间和三维空间中分别表现为一条直线和一个平面，以人类的认知无法具象表现三维以上的超平面，但可以用函数表示，以此类推到无穷维。在不考虑空间维数的情况下，这些线性函数被统称为超平面。当然在一个空间中存在不止一个超平面可以将样本分成两类，那么所有到超平面与最近的那部分样本点的距离之和最大的那个超平面被称为最优超平面，距离最近的那些样本点被称为支持向量。

## 线性可分（硬间隔最大化）

线性可分的分类函数如下：



其KKT条件为：



我们分析一下，对于任意的训练样本(xi,yi),

若αi=0，则其不会影响模型的训练；若αi>0，则yi·f(xi)-1=0，即yi·f(xi)=1，即该样本一定在边界上，是一个支持向量。

这里显示出了支持向量机的重要特征：当训练完成后，大部分样本都不需要保留，最终模型只与支持向量有关。